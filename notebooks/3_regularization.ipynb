{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the TensorFlow graph with L2 regularization\n",
    "\n",
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "l2_strength = 5e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_1  = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases_2  = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  relu_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "  relu_1 = tf.nn.relu(relu_1)\n",
    "  logits = tf.matmul(relu_1, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "  # Use L2 regularization\n",
    "  l2_loss = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "  loss += l2_strength*l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "  test_prediction  = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 1911.110718\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 20.5%\n",
      "Minibatch loss at step 500: 126.877388\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1000: 11.048361\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1500: 1.393824\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2000: 0.702677\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 0.623778\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 3000: 0.702779\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.1%\n",
      "Test accuracy: 91.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current accuracy (91,4%) is better than the previous with no regularization (88,6%). And the Test Accuracy is much higher than the minibatch/validation accuracy, so we can say the model is generalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 640\n",
      "Initialized\n",
      "Minibatch loss at step 0: 1870.241699\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 27.3%\n",
      "Minibatch loss at step 500: 128.591248\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 1000: 10.611886\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1500: 1.065916\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 2000: 0.275970\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 2500: 0.202047\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 3000: 0.190743\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.9%\n",
      "Test accuracy: 86.3%\n"
     ]
    }
   ],
   "source": [
    "batches = 5\n",
    "num_steps = 3001\n",
    "train_labels_small  = train_labels[0:(batch_size*batches), :]\n",
    "train_dataset_small = train_dataset[0:(batch_size*batches), :]\n",
    "\n",
    "print(\"Total training samples:\", train_dataset_small.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels_small.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset_small[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels_small[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 5 batches (640 samples) we can still get a pretty good Test accuracy. The model doesn't seem overfitted to the 5 minibatches because it can still find a general solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 395.956787\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 17.0%\n",
      "Minibatch loss at step 500: 16.826612\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1000: 7.952658\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1500: 5.441921\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 2000: 5.713160\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2500: 4.556868\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 3000: 7.306358\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.1%\n"
     ]
    }
   ],
   "source": [
    "# Train only with Dropout\n",
    "\n",
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "dropout_keep_prob = 0.7\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  keep_prob_1 = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_1  = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases_2  = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  relu_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "  relu_1 = tf.nn.relu(relu_1)\n",
    "  \n",
    "  # Apply dropout to the outputs of the first hidden layer\n",
    "  drop_1 = tf.nn.dropout(relu_1, keep_prob_1)\n",
    "    \n",
    "  logits = tf.matmul(drop_1, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "  test_prediction  = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob_1 : dropout_keep_prob}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with dropout doesn't produce a much higher score than the model without dropout. However, we can see that it seems to be generalizing the solution because  `[Test acc.] > [Validation acc.] > [Train acc.]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 640\n",
      "Initialized\n",
      "Minibatch loss at step 0: 424.628662\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 33.6%\n",
      "Minibatch loss at step 500: 2.400055\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.0%\n",
      "Test accuracy: 85.1%\n"
     ]
    }
   ],
   "source": [
    "# Extreme overfitting case\n",
    "batches = 5\n",
    "num_steps = 3001\n",
    "train_labels_small  = train_labels[0:(batch_size*batches), :]\n",
    "train_dataset_small = train_dataset[0:(batch_size*batches), :]\n",
    "\n",
    "print(\"Total training samples:\", train_dataset_small.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels_small.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset_small[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels_small[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob_1 : dropout_keep_prob}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the extreme overfitting case produces a general solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 1949.289062\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 32.3%\n",
      "Minibatch loss at step 500: 127.189072\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1000: 11.066168\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1500: 1.479852\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2000: 0.754274\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2500: 0.667811\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3000: 0.747018\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.6%\n",
      "Test accuracy: 91.1%\n"
     ]
    }
   ],
   "source": [
    "# Train with Dropout + L2 regularization\n",
    "\n",
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "dropout_keep_prob = 0.7\n",
    "l2_strength = 5e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  keep_prob_1 = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_1  = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases_2  = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  relu_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "  relu_1 = tf.nn.relu(relu_1)\n",
    "  \n",
    "  # Apply dropout to the outputs of the first hidden layer\n",
    "  drop_1 = tf.nn.dropout(relu_1, keep_prob_1)\n",
    "    \n",
    "  logits = tf.matmul(drop_1, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "  # Use L2 regularization\n",
    "  l2_loss = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "  loss += l2_strength*l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "  test_prediction  = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob_1 : dropout_keep_prob}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout doesn't seem to affect the results when applied to our model with L2 regularization. MNIST is a large dataset and that probably makes dropout unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our approach is to build a network similar to the one which got 97,1% test accuracy.\n",
    "# We also add L2 regularization (no dropout), decaying learning rate and use AdamOptimizer.\n",
    "# Other changes include the usage of ReLUs instead of tanh and exponential learning\n",
    "# rate decay instead of linear learning rate decay.\n",
    "\n",
    "batch_size = 128\n",
    "num_steps = 412 * train_labels.shape[0] / batch_size # 412 epochs\n",
    "hidden_nodes_1 = 1024\n",
    "hidden_nodes_2 = 300\n",
    "hidden_nodes_3 = 50\n",
    "l2_strength = 0 # 1e-6 # very weak because we are summing regularizers from many weight matrixes\n",
    "initial_learning_rate = 5e-6 # smaller because we are using more layers\n",
    "learning_rate_decay_steps = int(1 * train_labels.shape[0] / batch_size) # 1 epoch\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_1]))\n",
    "  weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_1, hidden_nodes_2]))\n",
    "  weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_2, hidden_nodes_3]))\n",
    "  weights_4 = tf.Variable(tf.truncated_normal([hidden_nodes_3, num_labels]))\n",
    "  biases_1  = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
    "  biases_2  = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
    "  biases_3  = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
    "  biases_4  = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  hidden_2 = tf.nn.relu(tf.matmul(hidden_1, weights_2) + biases_2)\n",
    "  hidden_3 = tf.nn.relu(tf.matmul(hidden_2, weights_3) + biases_3)\n",
    "\n",
    "  logits = tf.matmul(hidden_3, weights_4) + biases_4\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "  # Use L2 regularization\n",
    "  l2_loss = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) +\n",
    "             tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4))\n",
    "  loss += l2_strength * l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, learning_rate_decay_steps, 0.96)\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2), weights_3) + biases_3), weights_4) + biases_4)\n",
    "  test_prediction  = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2), weights_3) + biases_3), weights_4) + biases_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Total steps: 643750\n",
      "Steps per epoch: 1562\n",
      "Learning rate: 5e-06\n",
      "Minibatch loss at step 0: 24874.755859\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 13.6%\n",
      "Learning rate: 4.93509e-06\n",
      "Minibatch loss at step 500: 17314.750000\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 13.6%\n",
      "Learning rate: 4.87102e-06\n",
      "Minibatch loss at step 1000: 12671.798828\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 14.3%\n",
      "Learning rate: 4.80778e-06\n",
      "Minibatch loss at step 1500: 8809.632812\n",
      "Minibatch accuracy: 21.1%\n",
      "Validation accuracy: 17.9%\n",
      "Learning rate: 4.74537e-06\n",
      "Minibatch loss at step 2000: 6990.116699\n",
      "Minibatch accuracy: 20.3%\n",
      "Validation accuracy: 22.1%\n",
      "Learning rate: 4.68376e-06\n",
      "Minibatch loss at step 2500: 5583.059570\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 26.4%\n",
      "Learning rate: 4.62296e-06\n",
      "Minibatch loss at step 3000: 4710.575195\n",
      "Minibatch accuracy: 29.7%\n",
      "Validation accuracy: 30.6%\n",
      "Learning rate: 4.56294e-06\n",
      "Minibatch loss at step 3500: 4437.015137\n",
      "Minibatch accuracy: 33.6%\n",
      "Validation accuracy: 35.3%\n",
      "Learning rate: 4.5037e-06\n",
      "Minibatch loss at step 4000: 3952.441406\n",
      "Minibatch accuracy: 43.0%\n",
      "Validation accuracy: 39.8%\n",
      "Learning rate: 4.44524e-06\n",
      "Minibatch loss at step 4500: 2792.764404\n",
      "Minibatch accuracy: 42.2%\n",
      "Validation accuracy: 43.3%\n",
      "Learning rate: 4.38753e-06\n",
      "Minibatch loss at step 5000: 3133.949707\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 46.1%\n",
      "Learning rate: 4.33057e-06\n",
      "Minibatch loss at step 5500: 2395.530029\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 48.9%\n",
      "Learning rate: 4.27435e-06\n",
      "Minibatch loss at step 6000: 2611.349121\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 51.2%\n",
      "Learning rate: 4.21886e-06\n",
      "Minibatch loss at step 6500: 2090.445068\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 53.3%\n",
      "Learning rate: 4.16409e-06\n",
      "Minibatch loss at step 7000: 1965.553833\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 55.1%\n",
      "Learning rate: 4.11003e-06\n",
      "Minibatch loss at step 7500: 1583.449585\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 56.5%\n",
      "Learning rate: 4.05667e-06\n",
      "Minibatch loss at step 8000: 2695.886230\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 57.8%\n",
      "Learning rate: 4.004e-06\n",
      "Minibatch loss at step 8500: 1833.820068\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 58.9%\n",
      "Learning rate: 3.95202e-06\n",
      "Minibatch loss at step 9000: 1895.550781\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 59.9%\n",
      "Learning rate: 3.90072e-06\n",
      "Minibatch loss at step 9500: 1592.015747\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 60.6%\n",
      "Learning rate: 3.85008e-06\n",
      "Minibatch loss at step 10000: 2296.545898\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 61.4%\n",
      "Learning rate: 3.8001e-06\n",
      "Minibatch loss at step 10500: 2141.513184\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 62.2%\n",
      "Learning rate: 3.75076e-06\n",
      "Minibatch loss at step 11000: 1409.359009\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 62.7%\n",
      "Learning rate: 3.70207e-06\n",
      "Minibatch loss at step 11500: 1457.221680\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 63.2%\n",
      "Learning rate: 3.65401e-06\n",
      "Minibatch loss at step 12000: 1518.264893\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 63.7%\n",
      "Learning rate: 3.60657e-06\n",
      "Minibatch loss at step 12500: 1924.124023\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 64.1%\n",
      "Learning rate: 3.55975e-06\n",
      "Minibatch loss at step 13000: 1598.816528\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 64.5%\n",
      "Learning rate: 3.51354e-06\n",
      "Minibatch loss at step 13500: 1841.887817\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 64.8%\n",
      "Learning rate: 3.46792e-06\n",
      "Minibatch loss at step 14000: 1728.947266\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 65.1%\n",
      "Learning rate: 3.4229e-06\n",
      "Minibatch loss at step 14500: 1536.121338\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 65.4%\n",
      "Learning rate: 3.37846e-06\n",
      "Minibatch loss at step 15000: 2019.759033\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 65.5%\n",
      "Learning rate: 3.3346e-06\n",
      "Minibatch loss at step 15500: 1481.443848\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 65.8%\n",
      "Learning rate: 3.29131e-06\n",
      "Minibatch loss at step 16000: 1133.710693\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 66.0%\n",
      "Learning rate: 3.24858e-06\n",
      "Minibatch loss at step 16500: 1189.655762\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 66.2%\n",
      "Learning rate: 3.20641e-06\n",
      "Minibatch loss at step 17000: 1000.385315\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 66.5%\n",
      "Learning rate: 3.16478e-06\n",
      "Minibatch loss at step 17500: 1280.107788\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 66.8%\n",
      "Learning rate: 3.1237e-06\n",
      "Minibatch loss at step 18000: 1223.710449\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 66.9%\n",
      "Learning rate: 3.08315e-06\n",
      "Minibatch loss at step 18500: 1283.866577\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 67.1%\n",
      "Learning rate: 3.04312e-06\n",
      "Minibatch loss at step 19000: 1227.569092\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 67.2%\n",
      "Learning rate: 3.00361e-06\n",
      "Minibatch loss at step 19500: 1295.911621\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 67.4%\n",
      "Learning rate: 2.96462e-06\n",
      "Minibatch loss at step 20000: 1312.336670\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 67.7%\n",
      "Learning rate: 2.92613e-06\n",
      "Minibatch loss at step 20500: 1123.916870\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 67.9%\n",
      "Learning rate: 2.88814e-06\n",
      "Minibatch loss at step 21000: 867.036743\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 68.1%\n",
      "Learning rate: 2.85065e-06\n",
      "Minibatch loss at step 21500: 1273.345215\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 68.3%\n",
      "Learning rate: 2.81364e-06\n",
      "Minibatch loss at step 22000: 869.565369\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 68.3%\n",
      "Learning rate: 2.77712e-06\n",
      "Minibatch loss at step 22500: 1461.601440\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 68.5%\n",
      "Learning rate: 2.74106e-06\n",
      "Minibatch loss at step 23000: 1462.504150\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 68.6%\n",
      "Learning rate: 2.70548e-06\n",
      "Minibatch loss at step 23500: 1140.121338\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 68.7%\n",
      "Learning rate: 2.67035e-06\n",
      "Minibatch loss at step 24000: 1062.906250\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 68.8%\n",
      "Learning rate: 2.63569e-06\n",
      "Minibatch loss at step 24500: 1091.329346\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 69.0%\n",
      "Learning rate: 2.60147e-06\n",
      "Minibatch loss at step 25000: 813.311401\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 69.1%\n",
      "Learning rate: 2.5677e-06\n",
      "Minibatch loss at step 25500: 966.415771\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 69.2%\n",
      "Learning rate: 2.53436e-06\n",
      "Minibatch loss at step 26000: 1663.814697\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 69.2%\n",
      "Learning rate: 2.50146e-06\n",
      "Minibatch loss at step 26500: 1277.709839\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 69.3%\n",
      "Learning rate: 2.46899e-06\n",
      "Minibatch loss at step 27000: 1046.733643\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 69.3%\n",
      "Learning rate: 2.43693e-06\n",
      "Minibatch loss at step 27500: 1123.146484\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 69.5%\n",
      "Learning rate: 2.4053e-06\n",
      "Minibatch loss at step 28000: 847.256531\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 69.6%\n",
      "Learning rate: 2.37407e-06\n",
      "Minibatch loss at step 28500: 1039.196411\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 69.6%\n",
      "Learning rate: 2.34325e-06\n",
      "Minibatch loss at step 29000: 881.272339\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 69.6%\n",
      "Learning rate: 2.31283e-06\n",
      "Minibatch loss at step 29500: 1013.805786\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 69.6%\n",
      "Learning rate: 2.2828e-06\n",
      "Minibatch loss at step 30000: 690.823608\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 69.5%\n",
      "Learning rate: 2.25317e-06\n",
      "Minibatch loss at step 30500: 1002.647095\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 69.6%\n",
      "Learning rate: 2.22392e-06\n",
      "Minibatch loss at step 31000: 1162.796753\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 69.5%\n",
      "Learning rate: 2.19505e-06\n",
      "Minibatch loss at step 31500: 929.870117\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 69.6%\n",
      "Learning rate: 2.16655e-06\n",
      "Minibatch loss at step 32000: 1006.712524\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 69.5%\n",
      "Learning rate: 2.13842e-06\n",
      "Minibatch loss at step 32500: 1199.888672\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 69.5%\n",
      "Learning rate: 2.11066e-06\n",
      "Minibatch loss at step 33000: 877.950928\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 69.5%\n",
      "Learning rate: 2.08326e-06\n",
      "Minibatch loss at step 33500: 887.134399\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 69.5%\n",
      "Learning rate: 2.05621e-06\n",
      "Minibatch loss at step 34000: 955.360168\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 69.5%\n",
      "Learning rate: 2.02952e-06\n",
      "Minibatch loss at step 34500: 1058.599854\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 69.4%\n",
      "Learning rate: 2.00317e-06\n",
      "Minibatch loss at step 35000: 436.641541\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 69.3%\n",
      "Learning rate: 1.97717e-06\n",
      "Minibatch loss at step 35500: 880.307983\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 69.2%\n",
      "Learning rate: 1.9515e-06\n",
      "Minibatch loss at step 36000: 947.190308\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 69.1%\n",
      "Learning rate: 1.92616e-06\n",
      "Minibatch loss at step 36500: 681.260925\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 69.2%\n",
      "Learning rate: 1.90116e-06\n",
      "Minibatch loss at step 37000: 651.986450\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 69.1%\n",
      "Learning rate: 1.87648e-06\n",
      "Minibatch loss at step 37500: 795.760986\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 69.1%\n",
      "Learning rate: 1.85212e-06\n",
      "Minibatch loss at step 38000: 931.955933\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 69.1%\n",
      "Learning rate: 1.82807e-06\n",
      "Minibatch loss at step 38500: 727.737305\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 68.9%\n",
      "Learning rate: 1.80434e-06\n",
      "Minibatch loss at step 39000: 1009.531982\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 68.9%\n",
      "Learning rate: 1.78091e-06\n",
      "Minibatch loss at step 39500: 1143.536621\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 68.8%\n",
      "Learning rate: 1.75779e-06\n",
      "Minibatch loss at step 40000: 581.416626\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 68.9%\n",
      "Learning rate: 1.73497e-06\n",
      "Minibatch loss at step 40500: 804.584839\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 68.8%\n",
      "Learning rate: 1.71245e-06\n",
      "Minibatch loss at step 41000: 942.993530\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 68.8%\n",
      "Learning rate: 1.69022e-06\n",
      "Minibatch loss at step 41500: 878.726929\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 68.8%\n",
      "Learning rate: 1.66828e-06\n",
      "Minibatch loss at step 42000: 719.002197\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 68.7%\n",
      "Learning rate: 1.64662e-06\n",
      "Minibatch loss at step 42500: 590.349731\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 68.6%\n",
      "Learning rate: 1.62524e-06\n",
      "Minibatch loss at step 43000: 919.388672\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 68.7%\n",
      "Learning rate: 1.60414e-06\n",
      "Minibatch loss at step 43500: 420.483032\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 68.7%\n",
      "Learning rate: 1.58332e-06\n",
      "Minibatch loss at step 44000: 761.499817\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 68.7%\n",
      "Learning rate: 1.56276e-06\n",
      "Minibatch loss at step 44500: 802.887268\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 68.6%\n",
      "Learning rate: 1.54247e-06\n",
      "Minibatch loss at step 45000: 618.432617\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 68.7%\n",
      "Learning rate: 1.52245e-06\n",
      "Minibatch loss at step 45500: 538.029175\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 68.5%\n",
      "Learning rate: 1.50268e-06\n",
      "Minibatch loss at step 46000: 847.027710\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 68.5%\n",
      "Learning rate: 1.48318e-06\n",
      "Minibatch loss at step 46500: 716.253418\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 68.4%\n",
      "Learning rate: 1.46392e-06\n",
      "Minibatch loss at step 47000: 751.884094\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 68.4%\n",
      "Learning rate: 1.44492e-06\n",
      "Minibatch loss at step 47500: 418.784790\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 68.3%\n",
      "Learning rate: 1.42616e-06\n",
      "Minibatch loss at step 48000: 734.598389\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 68.2%\n",
      "Learning rate: 1.40764e-06\n",
      "Minibatch loss at step 48500: 466.828094\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 68.0%\n",
      "Learning rate: 1.38937e-06\n",
      "Minibatch loss at step 49000: 638.080383\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 68.0%\n",
      "Learning rate: 1.37133e-06\n",
      "Minibatch loss at step 49500: 633.077637\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 68.0%\n",
      "Learning rate: 1.35353e-06\n",
      "Minibatch loss at step 50000: 534.764893\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 67.9%\n",
      "Learning rate: 1.33596e-06\n",
      "Minibatch loss at step 50500: 499.424500\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 67.7%\n",
      "Learning rate: 1.31861e-06\n",
      "Minibatch loss at step 51000: 408.028870\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 67.6%\n",
      "Learning rate: 1.30149e-06\n",
      "Minibatch loss at step 51500: 589.236816\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 67.5%\n",
      "Learning rate: 1.2846e-06\n",
      "Minibatch loss at step 52000: 660.652588\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 67.4%\n",
      "Learning rate: 1.26792e-06\n",
      "Minibatch loss at step 52500: 452.279602\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 67.4%\n",
      "Learning rate: 1.25146e-06\n",
      "Minibatch loss at step 53000: 817.062622\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 67.4%\n",
      "Learning rate: 1.23521e-06\n",
      "Minibatch loss at step 53500: 472.206726\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 67.3%\n",
      "Learning rate: 1.21918e-06\n",
      "Minibatch loss at step 54000: 732.509705\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 67.2%\n",
      "Learning rate: 1.20335e-06\n",
      "Minibatch loss at step 54500: 609.404663\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 67.3%\n",
      "Learning rate: 1.18773e-06\n",
      "Minibatch loss at step 55000: 612.263977\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 67.2%\n",
      "Learning rate: 1.17231e-06\n",
      "Minibatch loss at step 55500: 570.596436\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 67.2%\n",
      "Learning rate: 1.15709e-06\n",
      "Minibatch loss at step 56000: 458.627502\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 67.2%\n",
      "Learning rate: 1.14207e-06\n",
      "Minibatch loss at step 56500: 499.684692\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 67.1%\n",
      "Learning rate: 1.12724e-06\n",
      "Minibatch loss at step 57000: 739.006897\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 67.0%\n",
      "Learning rate: 1.11261e-06\n",
      "Minibatch loss at step 57500: 454.207031\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 67.0%\n",
      "Learning rate: 1.09816e-06\n",
      "Minibatch loss at step 58000: 679.732300\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 66.9%\n",
      "Learning rate: 1.08391e-06\n",
      "Minibatch loss at step 58500: 689.289307\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 66.8%\n",
      "Learning rate: 1.06984e-06\n",
      "Minibatch loss at step 59000: 601.423462\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 66.7%\n",
      "Learning rate: 1.05595e-06\n",
      "Minibatch loss at step 59500: 349.043518\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 66.6%\n",
      "Learning rate: 1.04224e-06\n",
      "Minibatch loss at step 60000: 621.200562\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 66.5%\n",
      "Learning rate: 1.02871e-06\n",
      "Minibatch loss at step 60500: 638.616211\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 66.6%\n",
      "Learning rate: 1.01535e-06\n",
      "Minibatch loss at step 61000: 650.419922\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 66.4%\n",
      "Learning rate: 1.00217e-06\n",
      "Minibatch loss at step 61500: 283.772461\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 66.4%\n",
      "Learning rate: 9.89161e-07\n",
      "Minibatch loss at step 62000: 606.093628\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 66.5%\n",
      "Learning rate: 9.76319e-07\n",
      "Minibatch loss at step 62500: 566.898254\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 66.4%\n",
      "Learning rate: 9.63645e-07\n",
      "Minibatch loss at step 63000: 486.349548\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 66.4%\n",
      "Learning rate: 9.51134e-07\n",
      "Minibatch loss at step 63500: 644.971436\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 66.4%\n",
      "Learning rate: 9.38787e-07\n",
      "Minibatch loss at step 64000: 242.453964\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 66.3%\n",
      "Learning rate: 9.26599e-07\n",
      "Minibatch loss at step 64500: 454.414734\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 66.2%\n",
      "Learning rate: 9.1457e-07\n",
      "Minibatch loss at step 65000: 348.531952\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 66.2%\n",
      "Learning rate: 9.02697e-07\n",
      "Minibatch loss at step 65500: 477.675323\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 66.0%\n",
      "Learning rate: 8.90978e-07\n",
      "Minibatch loss at step 66000: 505.332489\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 66.0%\n",
      "Learning rate: 8.79411e-07\n",
      "Minibatch loss at step 66500: 505.444183\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 65.9%\n",
      "Learning rate: 8.67994e-07\n",
      "Minibatch loss at step 67000: 612.849365\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 65.9%\n",
      "Learning rate: 8.56725e-07\n",
      "Minibatch loss at step 67500: 351.227539\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 65.7%\n",
      "Learning rate: 8.45603e-07\n",
      "Minibatch loss at step 68000: 451.247559\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 65.6%\n",
      "Learning rate: 8.34625e-07\n",
      "Minibatch loss at step 68500: 427.937805\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 65.6%\n",
      "Learning rate: 8.2379e-07\n",
      "Minibatch loss at step 69000: 324.350220\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 65.5%\n",
      "Learning rate: 8.13095e-07\n",
      "Minibatch loss at step 69500: 497.877625\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 65.4%\n",
      "Learning rate: 8.0254e-07\n",
      "Minibatch loss at step 70000: 533.114807\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 65.4%\n",
      "Learning rate: 7.92121e-07\n",
      "Minibatch loss at step 70500: 469.060120\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 65.3%\n",
      "Learning rate: 7.81838e-07\n",
      "Minibatch loss at step 71000: 307.105286\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 65.2%\n",
      "Learning rate: 7.71688e-07\n",
      "Minibatch loss at step 71500: 475.472870\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 65.1%\n",
      "Learning rate: 7.61669e-07\n",
      "Minibatch loss at step 72000: 361.824677\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 65.0%\n",
      "Learning rate: 7.51781e-07\n",
      "Minibatch loss at step 72500: 464.852844\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 64.9%\n",
      "Learning rate: 7.42021e-07\n",
      "Minibatch loss at step 73000: 355.686157\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 64.8%\n",
      "Learning rate: 7.32388e-07\n",
      "Minibatch loss at step 73500: 625.374939\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 64.7%\n",
      "Learning rate: 7.2288e-07\n",
      "Minibatch loss at step 74000: 500.429932\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 64.5%\n",
      "Learning rate: 7.13496e-07\n",
      "Minibatch loss at step 74500: 324.678955\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 64.5%\n",
      "Learning rate: 7.04233e-07\n",
      "Minibatch loss at step 75000: 412.830627\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 64.4%\n",
      "Learning rate: 6.9509e-07\n",
      "Minibatch loss at step 75500: 579.629028\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.4%\n",
      "Learning rate: 6.86067e-07\n",
      "Minibatch loss at step 76000: 300.246033\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 64.3%\n",
      "Learning rate: 6.7716e-07\n",
      "Minibatch loss at step 76500: 495.156219\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 64.2%\n",
      "Learning rate: 6.68369e-07\n",
      "Minibatch loss at step 77000: 399.949646\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 64.2%\n",
      "Learning rate: 6.59692e-07\n",
      "Minibatch loss at step 77500: 368.558594\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 64.1%\n",
      "Learning rate: 6.51128e-07\n",
      "Minibatch loss at step 78000: 301.643097\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 64.2%\n",
      "Learning rate: 6.42675e-07\n",
      "Minibatch loss at step 78500: 239.186417\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 64.2%\n",
      "Learning rate: 6.34331e-07\n",
      "Minibatch loss at step 79000: 309.651184\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 64.1%\n",
      "Learning rate: 6.26096e-07\n",
      "Minibatch loss at step 79500: 308.697266\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.0%\n",
      "Learning rate: 6.17968e-07\n",
      "Minibatch loss at step 80000: 467.390625\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 64.0%\n",
      "Learning rate: 6.09946e-07\n",
      "Minibatch loss at step 80500: 387.834534\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 63.9%\n",
      "Learning rate: 6.02027e-07\n",
      "Minibatch loss at step 81000: 633.882080\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 63.7%\n",
      "Learning rate: 5.94211e-07\n",
      "Minibatch loss at step 81500: 336.048584\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 63.6%\n",
      "Learning rate: 5.86497e-07\n",
      "Minibatch loss at step 82000: 363.521484\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 63.6%\n",
      "Learning rate: 5.78883e-07\n",
      "Minibatch loss at step 82500: 553.012817\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 63.6%\n",
      "Learning rate: 5.71368e-07\n",
      "Minibatch loss at step 83000: 319.022369\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 63.7%\n",
      "Learning rate: 5.6395e-07\n",
      "Minibatch loss at step 83500: 553.710815\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 63.6%\n",
      "Learning rate: 5.56629e-07\n",
      "Minibatch loss at step 84000: 237.407013\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 63.5%\n",
      "Learning rate: 5.49403e-07\n",
      "Minibatch loss at step 84500: 541.729797\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 63.4%\n",
      "Learning rate: 5.4227e-07\n",
      "Minibatch loss at step 85000: 398.814880\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 63.4%\n",
      "Learning rate: 5.3523e-07\n",
      "Minibatch loss at step 85500: 325.061401\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 63.4%\n",
      "Learning rate: 5.28282e-07\n",
      "Minibatch loss at step 86000: 184.110962\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 63.3%\n",
      "Learning rate: 5.21424e-07\n",
      "Minibatch loss at step 86500: 335.418213\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 63.3%\n",
      "Learning rate: 5.14654e-07\n",
      "Minibatch loss at step 87000: 436.033539\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 63.2%\n",
      "Learning rate: 5.07973e-07\n",
      "Minibatch loss at step 87500: 323.968506\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 63.2%\n",
      "Learning rate: 5.01378e-07\n",
      "Minibatch loss at step 88000: 413.683990\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 63.1%\n",
      "Learning rate: 4.94869e-07\n",
      "Minibatch loss at step 88500: 411.012085\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 63.0%\n",
      "Learning rate: 4.88445e-07\n",
      "Minibatch loss at step 89000: 416.587555\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 63.0%\n",
      "Learning rate: 4.82104e-07\n",
      "Minibatch loss at step 89500: 386.779480\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 62.9%\n",
      "Learning rate: 4.75845e-07\n",
      "Minibatch loss at step 90000: 238.671234\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 62.8%\n",
      "Learning rate: 4.69668e-07\n",
      "Minibatch loss at step 90500: 654.198303\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 62.7%\n",
      "Learning rate: 4.6357e-07\n",
      "Minibatch loss at step 91000: 233.641418\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 62.7%\n",
      "Learning rate: 4.57552e-07\n",
      "Minibatch loss at step 91500: 372.590363\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 62.7%\n",
      "Learning rate: 4.51612e-07\n",
      "Minibatch loss at step 92000: 335.904907\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 62.7%\n",
      "Learning rate: 4.45749e-07\n",
      "Minibatch loss at step 92500: 241.349548\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 62.6%\n",
      "Learning rate: 4.39962e-07\n",
      "Minibatch loss at step 93000: 294.850708\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 62.6%\n",
      "Learning rate: 4.34251e-07\n",
      "Minibatch loss at step 93500: 217.915726\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 62.5%\n",
      "Learning rate: 4.28613e-07\n",
      "Minibatch loss at step 94000: 187.839752\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 62.4%\n",
      "Learning rate: 4.23049e-07\n",
      "Minibatch loss at step 94500: 431.355499\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 62.3%\n",
      "Learning rate: 4.17557e-07\n",
      "Minibatch loss at step 95000: 418.202759\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 62.3%\n",
      "Learning rate: 4.12136e-07\n",
      "Minibatch loss at step 95500: 335.068054\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 62.2%\n",
      "Learning rate: 4.06785e-07\n",
      "Minibatch loss at step 96000: 293.689270\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 62.1%\n",
      "Learning rate: 4.01504e-07\n",
      "Minibatch loss at step 96500: 251.188171\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 62.1%\n",
      "Learning rate: 3.96292e-07\n",
      "Minibatch loss at step 97000: 350.747742\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 62.0%\n",
      "Learning rate: 3.91147e-07\n",
      "Minibatch loss at step 97500: 543.555603\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 62.0%\n",
      "Learning rate: 3.86069e-07\n",
      "Minibatch loss at step 98000: 429.072754\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 61.9%\n",
      "Learning rate: 3.81057e-07\n",
      "Minibatch loss at step 98500: 349.067810\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 61.8%\n",
      "Learning rate: 3.7611e-07\n",
      "Minibatch loss at step 99000: 330.880493\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 61.7%\n",
      "Learning rate: 3.71227e-07\n",
      "Minibatch loss at step 99500: 469.559662\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 61.7%\n",
      "Learning rate: 3.66408e-07\n",
      "Minibatch loss at step 100000: 514.384033\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 61.6%\n",
      "Learning rate: 3.61651e-07\n",
      "Minibatch loss at step 100500: 404.498413\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 61.6%\n",
      "Learning rate: 3.56956e-07\n",
      "Minibatch loss at step 101000: 306.207367\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 61.6%\n",
      "Learning rate: 3.52322e-07\n",
      "Minibatch loss at step 101500: 263.090820\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 61.5%\n",
      "Learning rate: 3.47748e-07\n",
      "Minibatch loss at step 102000: 394.848358\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 61.5%\n",
      "Learning rate: 3.43234e-07\n",
      "Minibatch loss at step 102500: 377.932648\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 61.4%\n",
      "Learning rate: 3.38778e-07\n",
      "Minibatch loss at step 103000: 401.088531\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 61.4%\n",
      "Learning rate: 3.3438e-07\n",
      "Minibatch loss at step 103500: 250.239273\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 61.3%\n",
      "Learning rate: 3.30039e-07\n",
      "Minibatch loss at step 104000: 378.821014\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 61.3%\n",
      "Learning rate: 3.25754e-07\n",
      "Minibatch loss at step 104500: 295.601990\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 61.2%\n",
      "Learning rate: 3.21525e-07\n",
      "Minibatch loss at step 105000: 353.395782\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 61.2%\n",
      "Learning rate: 3.17351e-07\n",
      "Minibatch loss at step 105500: 192.994049\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 61.2%\n",
      "Learning rate: 3.13231e-07\n",
      "Minibatch loss at step 106000: 379.158997\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 61.1%\n",
      "Learning rate: 3.09165e-07\n",
      "Minibatch loss at step 106500: 311.095947\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 61.1%\n",
      "Learning rate: 3.05151e-07\n",
      "Minibatch loss at step 107000: 367.535522\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 61.0%\n",
      "Learning rate: 3.01189e-07\n",
      "Minibatch loss at step 107500: 316.556396\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 61.0%\n",
      "Learning rate: 2.97279e-07\n",
      "Minibatch loss at step 108000: 281.147369\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 60.9%\n",
      "Learning rate: 2.9342e-07\n",
      "Minibatch loss at step 108500: 290.234528\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 60.8%\n",
      "Learning rate: 2.89611e-07\n",
      "Minibatch loss at step 109000: 362.713745\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 60.8%\n",
      "Learning rate: 2.85851e-07\n",
      "Minibatch loss at step 109500: 327.145752\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 60.9%\n",
      "Learning rate: 2.8214e-07\n",
      "Minibatch loss at step 110000: 307.394104\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 60.9%\n",
      "Learning rate: 2.78477e-07\n",
      "Minibatch loss at step 110500: 263.493317\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 60.8%\n",
      "Learning rate: 2.74862e-07\n",
      "Minibatch loss at step 111000: 270.350647\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 60.8%\n",
      "Learning rate: 2.71294e-07\n",
      "Minibatch loss at step 111500: 325.100342\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 60.8%\n",
      "Learning rate: 2.67772e-07\n",
      "Minibatch loss at step 112000: 343.464020\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 60.7%\n",
      "Learning rate: 2.64295e-07\n",
      "Minibatch loss at step 112500: 354.065613\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 60.7%\n",
      "Learning rate: 2.60864e-07\n",
      "Minibatch loss at step 113000: 407.031616\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 60.7%\n",
      "Learning rate: 2.57478e-07\n",
      "Minibatch loss at step 113500: 293.008087\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 60.6%\n",
      "Learning rate: 2.54135e-07\n",
      "Minibatch loss at step 114000: 245.035233\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 60.6%\n",
      "Learning rate: 2.50836e-07\n",
      "Minibatch loss at step 114500: 410.138489\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 60.6%\n",
      "Learning rate: 2.47579e-07\n",
      "Minibatch loss at step 115000: 323.054016\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 60.6%\n",
      "Learning rate: 2.44365e-07\n",
      "Minibatch loss at step 115500: 390.225647\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 60.6%\n",
      "Learning rate: 2.41193e-07\n",
      "Minibatch loss at step 116000: 354.403442\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 60.6%\n",
      "Learning rate: 2.38062e-07\n",
      "Minibatch loss at step 116500: 279.161011\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 60.5%\n",
      "Learning rate: 2.34971e-07\n",
      "Minibatch loss at step 117000: 429.234863\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 60.5%\n",
      "Learning rate: 2.31921e-07\n",
      "Minibatch loss at step 117500: 251.608322\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 60.5%\n",
      "Learning rate: 2.2891e-07\n",
      "Minibatch loss at step 118000: 207.031128\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 60.5%\n",
      "Learning rate: 2.25938e-07\n",
      "Minibatch loss at step 118500: 467.459717\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 60.5%\n",
      "Learning rate: 2.23005e-07\n",
      "Minibatch loss at step 119000: 229.613480\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 60.4%\n",
      "Learning rate: 2.2011e-07\n",
      "Minibatch loss at step 119500: 195.472870\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 60.4%\n",
      "Learning rate: 2.17252e-07\n",
      "Minibatch loss at step 120000: 369.248871\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 60.3%\n",
      "Learning rate: 2.14432e-07\n",
      "Minibatch loss at step 120500: 278.413483\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 60.3%\n",
      "Learning rate: 2.11648e-07\n",
      "Minibatch loss at step 121000: 186.334534\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 2.089e-07\n",
      "Minibatch loss at step 121500: 342.355927\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 2.06188e-07\n",
      "Minibatch loss at step 122000: 354.067749\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 2.03512e-07\n",
      "Minibatch loss at step 122500: 207.703400\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 2.00869e-07\n",
      "Minibatch loss at step 123000: 238.302277\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 1.98262e-07\n",
      "Minibatch loss at step 123500: 226.600037\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 1.95688e-07\n",
      "Minibatch loss at step 124000: 308.752319\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 1.93147e-07\n",
      "Minibatch loss at step 124500: 149.173538\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 1.9064e-07\n",
      "Minibatch loss at step 125000: 410.414764\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 1.88165e-07\n",
      "Minibatch loss at step 125500: 214.378174\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 60.2%\n",
      "Learning rate: 1.85722e-07\n",
      "Minibatch loss at step 126000: 235.924988\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 60.1%\n",
      "Learning rate: 1.83311e-07\n",
      "Minibatch loss at step 126500: 218.392303\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 60.1%\n",
      "Learning rate: 1.80931e-07\n",
      "Minibatch loss at step 127000: 263.689484\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 60.1%\n",
      "Learning rate: 1.78582e-07\n",
      "Minibatch loss at step 127500: 202.933807\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 60.1%\n",
      "Learning rate: 1.76264e-07\n",
      "Minibatch loss at step 128000: 259.621674\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 60.0%\n",
      "Learning rate: 1.73976e-07\n",
      "Minibatch loss at step 128500: 203.372894\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 60.0%\n",
      "Learning rate: 1.71717e-07\n",
      "Minibatch loss at step 129000: 284.415833\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 60.0%\n",
      "Learning rate: 1.69488e-07\n",
      "Minibatch loss at step 129500: 397.631897\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 60.0%\n",
      "Learning rate: 1.67288e-07\n",
      "Minibatch loss at step 130000: 180.440063\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 59.9%\n",
      "Learning rate: 1.65116e-07\n",
      "Minibatch loss at step 130500: 257.627045\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 59.9%\n",
      "Learning rate: 1.62972e-07\n",
      "Minibatch loss at step 131000: 328.488953\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 59.9%\n",
      "Learning rate: 1.60856e-07\n",
      "Minibatch loss at step 131500: 352.567474\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 59.9%\n",
      "Learning rate: 1.58768e-07\n",
      "Minibatch loss at step 132000: 475.544861\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 59.9%\n",
      "Learning rate: 1.56707e-07\n",
      "Minibatch loss at step 132500: 250.542282\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 59.8%\n",
      "Learning rate: 1.54673e-07\n",
      "Minibatch loss at step 133000: 188.038910\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 59.9%\n",
      "Learning rate: 1.52665e-07\n",
      "Minibatch loss at step 133500: 357.008118\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 59.8%\n",
      "Learning rate: 1.50683e-07\n",
      "Minibatch loss at step 134000: 252.965332\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 59.8%\n",
      "Learning rate: 1.48727e-07\n",
      "Minibatch loss at step 134500: 287.922058\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 59.8%\n",
      "Learning rate: 1.46796e-07\n",
      "Minibatch loss at step 135000: 306.300598\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 59.8%\n",
      "Learning rate: 1.4489e-07\n",
      "Minibatch loss at step 135500: 192.191345\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 59.8%\n",
      "Learning rate: 1.43009e-07\n",
      "Minibatch loss at step 136000: 340.580139\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 59.8%\n",
      "Learning rate: 1.41152e-07\n",
      "Minibatch loss at step 136500: 418.568542\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 59.8%\n",
      "Learning rate: 1.3932e-07\n",
      "Minibatch loss at step 137000: 165.228851\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 59.7%\n",
      "Learning rate: 1.37511e-07\n",
      "Minibatch loss at step 137500: 273.603546\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 59.7%\n",
      "Learning rate: 1.35726e-07\n",
      "Minibatch loss at step 138000: 319.523071\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 59.6%\n",
      "Learning rate: 1.33964e-07\n",
      "Minibatch loss at step 138500: 121.846565\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 59.6%\n",
      "Learning rate: 1.32225e-07\n",
      "Minibatch loss at step 139000: 218.253647\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 59.6%\n",
      "Learning rate: 1.30508e-07\n",
      "Minibatch loss at step 139500: 247.322693\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 59.6%\n",
      "Learning rate: 1.28814e-07\n",
      "Minibatch loss at step 140000: 259.358673\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.27142e-07\n",
      "Minibatch loss at step 140500: 385.244659\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.25491e-07\n",
      "Minibatch loss at step 141000: 221.608749\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.23862e-07\n",
      "Minibatch loss at step 141500: 227.453583\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.22254e-07\n",
      "Minibatch loss at step 142000: 258.771881\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.20667e-07\n",
      "Minibatch loss at step 142500: 377.885681\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.191e-07\n",
      "Minibatch loss at step 143000: 252.945541\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.17554e-07\n",
      "Minibatch loss at step 143500: 367.853699\n",
      "Minibatch accuracy: 49.2%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.16028e-07\n",
      "Minibatch loss at step 144000: 262.843079\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.14522e-07\n",
      "Minibatch loss at step 144500: 249.037796\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.13035e-07\n",
      "Minibatch loss at step 145000: 359.041931\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.11568e-07\n",
      "Minibatch loss at step 145500: 354.731873\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 59.5%\n",
      "Learning rate: 1.10119e-07\n",
      "Minibatch loss at step 146000: 160.232437\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 59.4%\n",
      "Learning rate: 1.0869e-07\n",
      "Minibatch loss at step 146500: 315.553772\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 59.4%\n",
      "Learning rate: 1.07279e-07\n",
      "Minibatch loss at step 147000: 187.475403\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 59.4%\n",
      "Learning rate: 1.05886e-07\n",
      "Minibatch loss at step 147500: 163.604385\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 59.4%\n",
      "Learning rate: 1.04511e-07\n",
      "Minibatch loss at step 148000: 488.144409\n",
      "Minibatch accuracy: 55.5%\n",
      "Validation accuracy: 59.4%\n",
      "Learning rate: 1.03154e-07\n",
      "Minibatch loss at step 148500: 496.424835\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 59.3%\n",
      "Learning rate: 1.01815e-07\n",
      "Minibatch loss at step 149000: 489.928528\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 59.3%\n",
      "Learning rate: 1.00493e-07\n",
      "Minibatch loss at step 149500: 431.881836\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 59.3%\n",
      "Learning rate: 9.91888e-08\n",
      "Minibatch loss at step 150000: 288.870941\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 59.3%\n",
      "Learning rate: 9.79011e-08\n",
      "Minibatch loss at step 150500: 330.293701\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 59.3%\n",
      "Learning rate: 9.66301e-08\n",
      "Minibatch loss at step 151000: 273.286835\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 59.3%\n",
      "Learning rate: 9.53757e-08\n",
      "Minibatch loss at step 151500: 331.586639\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 59.3%\n",
      "Learning rate: 9.41375e-08\n",
      "Minibatch loss at step 152000: 381.818115\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 59.3%\n",
      "Learning rate: 9.29154e-08\n",
      "Minibatch loss at step 152500: 289.195526\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 59.2%\n",
      "Learning rate: 9.17091e-08\n",
      "Minibatch loss at step 153000: 361.849487\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 59.2%\n",
      "Learning rate: 9.05185e-08\n",
      "Minibatch loss at step 153500: 235.363495\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 59.2%\n",
      "Learning rate: 8.93434e-08\n",
      "Minibatch loss at step 154000: 382.016266\n",
      "Minibatch accuracy: 49.2%\n",
      "Validation accuracy: 59.2%\n",
      "Learning rate: 8.81835e-08\n",
      "Minibatch loss at step 154500: 176.504898\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 59.2%\n",
      "Learning rate: 8.70387e-08\n",
      "Minibatch loss at step 155000: 228.762482\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 59.2%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-146dd2305e8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[1;32m---> 18\u001b[1;33m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m       \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Learning rate:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 564\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 637\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    638\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    642\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 628\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  print(\"Total steps:\", num_steps)\n",
    "  print(\"Steps per epoch:\", train_labels.shape[0] / batch_size)\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Learning rate:\", lr)\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tested dozens of network architectures and hyperparameters. Unfortunately none of them reached a better test accuracy than the 1 hidden layer network with L2 regularization.\n",
    "\n",
    "So we have just implemented a network based on the \"97,1% test accuracy model\" with some hyperparameters chosen by us.\n",
    "\n",
    "I think we have a problem with unstable gradients, as explained at http://neuralnetworksanddeeplearning.com/chap5.html and visualized at https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html.\n",
    "\n",
    "Also worth noting. We have found several cases where the loss decreases, but the accuracy doesn't get better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
